<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Self-Attention Mimicking Gradient Descent | Swayam Singh</title> <meta name="author" content="Swayam Singh"> <meta name="description" content="A linear self-attention layer in a Transformer architecture can mimic a single step of gradient descent for a linear regression task."> <meta name="keywords" content="academic-website, portfolio-website"> <meta property="og:site_name" content="Swayam Singh"> <meta property="og:type" content="article"> <meta property="og:title" content="Swayam Singh | Self-Attention Mimicking Gradient Descent"> <meta property="og:url" content="https://swayaminsync.github.io/blog/2023/test/"> <meta property="og:description" content="A linear self-attention layer in a Transformer architecture can mimic a single step of gradient descent for a linear regression task."> <meta property="og:image" content="https://swayaminsync.github.io/assets/img/prof_pic.jpg"> <meta property="og:locale" content="en"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="Self-Attention Mimicking Gradient Descent"> <meta name="twitter:description" content="A linear self-attention layer in a Transformer architecture can mimic a single step of gradient descent for a linear regression task."> <meta name="twitter:image" content="https://swayaminsync.github.io/assets/img/prof_pic.jpg"> <meta name="twitter:site" content="@_s_w_a_y_a_m_"> <meta name="twitter:creator" content="@_s_w_a_y_a_m_"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%9A%80&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://swayaminsync.github.io/blog/2023/test/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Swayam¬†</span>Singh</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Self-Attention Mimicking Gradient Descent</h1> <p class="post-meta">October 14, 2023</p> <p class="post-tags"> <a href="/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> ¬† ¬∑ ¬† <a href="/blog/tag/nlp"> <i class="fas fa-hashtag fa-sm"></i> NLP</a> ¬† ¬† ¬∑ ¬† <a href="/blog/category/transformers"> <i class="fas fa-tag fa-sm"></i> Transformers</a> ¬† </p> </header> <article class="post-content"> <div id="markdown-content"> <h1 id="self-attention-mimicking-gradient-descent">Self-Attention Mimicking Gradient Descent</h1> <hr> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/sample-post-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/sample-post-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/sample-post-1400.webp"></source> <img src="/assets/img/sample-post.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>This section of paper <strong><a href="https://arxiv.org/abs/2309.05858" rel="external nofollow noopener" target="_blank">Uncovering mesa-optimization algorithms in Transformers</a></strong> presents a theoretical construction where a linear self-attention layer in a Transformer architecture can mimic a single step of gradient descent for a linear regression task.</p> <h3 id="token-construction-from-the-paper">Token Construction (from the paper):</h3> <hr> <ul> <li> <strong>Tokens</strong>: A set of tokens \(E_T\) is constructed with \(T = N\) such that \(e_t = (y_{\tau,i}, x_{\tau,i})\) , where \(y_{\tau,i}\) and \(x_{\tau,i}\) are concatenated.</li> <li> <strong>Query Token</strong>: A query token \(e_{T+1}\) is created as \(e_{T+1} = (-W_0 x_{\tau,\text{test}}, x_{\tau,\text{test}})\) . This token represents the test input for which a prediction is to be made.</li> </ul> <blockquote> <p><strong>Why doing \(-W_0 x_{\tau,\text{test}}\) ?</strong> \(W_o\) represent the model‚Äôs initial weights, multiplying it with \(x_{\tau,\text{test}}\) provides initial context for prediction (basically giving a perspective to start with). The \(-_(ve)\) sign is to align with the GD update, where we move in the direction opposite to the gradient. So we can say that <strong><em>The initial negative prediction in the query token provides a starting point, and the self-attention mechanism‚Äôs update to this prediction results in a new prediction that mimics</em></strong></p> </blockquote> <h3 id="conditions-from-the-paper">Conditions (from the paper):</h3> <hr> <ol> <li>All bias terms are zero <ol> <li>basically only using the model weights without any bias term</li> </ol> </li> <li> \[W^T_k W_q = \begin{bmatrix} 0 &amp; 0 \\ 0 &amp; I_x \end{bmatrix}\] <ol> <li>As per my understanding, the relevance of this condition depends is as following <ol> <li> <p>So, I‚Äôll write the equations for calculating attention weights and they are self-explanatory to be honest (<strong>also this is my fav interpretation of this condition</strong>)</p> \[e = \begin{bmatrix} y_1 \\ x_1 \\ x_2 \end{bmatrix} \\ \text{ and }q = \begin{bmatrix} q_y \\ q_{x1} \\ q_{x2} \end{bmatrix}\] \[k = W_k e\] \[q' = W_q q\] \[\text{attention weight} \propto k^T q'\] <p>Given the condition, this becomes: \(\text{weight} \propto e^T W^T_k W_q q\)</p> <p>Now, plugging in the condition</p> \[W^T_k W_q = \begin{bmatrix} 0 &amp; 0 \\ 0 &amp; I_x \end{bmatrix}\] <p>the interaction simplifies to: \(\text{weight} \propto \begin{bmatrix} y_1 &amp; x_1 &amp; x_2 \end{bmatrix} \begin{bmatrix} 0 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0\\ 0 &amp; 0 &amp; 1 \end{bmatrix} \begin{bmatrix} q_y \\ q_{x1} \\ q_{x2} \end{bmatrix}\) \(\text{weight} \propto \begin{bmatrix} 0 &amp; x_1 &amp; x_2 \end{bmatrix} \begin{bmatrix} q_y \\ q_{x1} \\ q_{x2} \end{bmatrix}\) \(\text{weight} \propto x_1 q_{x1} + x_2 q_{x2}\)</p> <p>So what we can see here is that, the attention-weights are proportional to the dot-product of x-component of inputs and query (rejecting the influence of y-component)</p> </li> </ol> </li> <li> \[P W_v = \begin{bmatrix} -\eta I_y &amp; \eta W_0 \\ 0 &amp; 0 \end{bmatrix}\] </li> </ol> <p>This is the another condition and lets try to understand what it means, here \(\eta\) is the learning-rate, \(*P*\) is a projection matrix, and \(*W_v*\) is the weight matrix for the ‚Äúvalues‚Äù.</p> <p>Before diving into the interpretation of this condition, we should know that the <strong>Linear Self Attention</strong> can be represented as \(P V_{t} K^T_{t} q_{t}\) (considering 1 head for simplicity) here \(P\) is just the <strong>Projection matrix</strong> and the rest of the term is exact same as calculating attention.</p> <p>The matrix product \(PW_v\) determines how the values contribute to the updated query token. Now lets try to understand it</p> <p>In this interpretation, I‚Äôll again try to resolve this by solving equations, since our new <strong>**</strong><strong>**</strong><strong>value_matrix is \(PW_v\) i.e we can calculate values of input token as</strong></p> \[v = PW_ve\] <p>where \(e = \begin{bmatrix} y_1 \\ x_1 \\ x_2 \end{bmatrix}\) and \(P W_v = \begin{bmatrix} -\eta &amp; \eta W_{01} &amp; \eta W_{02} \\ 0 &amp; 0 &amp; 0\\ 0 &amp; 0 &amp; 0\end{bmatrix}\)</p> \[v = -\eta y + \eta W_{01}x_1 +\eta W_{02}x_2\] <p>Based on this equation we can interpret the condition as:</p> <ul> <li> <strong>The Upper Left Block \((-\eta I_y)\)</strong>: This block scales the y-component of the values (the outputs) by ‚àí<em>Œ∑.</em> In the context of gradient descent, the update is proportional to the negative gradient. This block captures the idea that the update to our model‚Äôs prediction should be in the opposite direction of the error (difference between prediction and actual output). Multiplying by ‚àí<em>Œ∑</em> ensures that if our model‚Äôs prediction is too high, it gets adjusted downwards, and if it‚Äôs too low, it gets adjusted upwards.</li> <li> <strong>The Upper Right Block (\(\eta W_0\))</strong>: This block scales the x-component of the values (the inputs) and then multiplies by the initial weight $<em>W_0$.</em> This captures the contribution of the inputs to the gradient of the loss with respect to the model parameters. In other words, it represents how much each input feature contributes to the error. Multiplying by \(*W_0*\) gives the model‚Äôs initial reliance on each feature, and the entire product indicates how the model should adjust its reliance on each feature to minimize the error.</li> <li> <strong>The Lower Blocks (0 matrices)</strong>: These blocks ensure that the x-component of the updated query token remains unchanged. This is consistent with the idea that the input part of our test example doesn‚Äôt change; only our model‚Äôs prediction (or representation) of it does.</li> </ul> </li> </ol> <p>That covers all the required conditions, now let‚Äôs see that satisfying these conditions, how the weights of the self-attention will contain the gradient of the loss of a Linear Regression objective.</p> <h3 id="gradient-descent-in-linear-regression">Gradient Descent in Linear Regression:</h3> <hr> <p>For a linear regression task, the gradient descent update rule is: \(\Delta W_0 = \eta \sum_{i=1}^{N} (y_{\tau,i} - W_0 x_{\tau,i}) x^T_{\tau,i}\)</p> <h3 id="linear-self-attention-layer">Linear Self-Attention Layer:</h3> <hr> <p>Given the conditions, the self-attention mechanism computes the weighted sum of values based on the similarity (dot product) of the query with the keys.</p> <p><strong>To prove:</strong> The weights in the attention mechanism will effectively compute the gradient of the loss with respect to the model parameters.</p> <p>Let‚Äôs move to the final derivation:</p> <ol> <li> <strong>Attention Weights Calculation</strong>: Given the second condition, the attention weights are determined by the dot product of the query with the keys: \(\alpha_t = q \cdot k_t\) Where \(q\) is the query, and $k_t$ is the key for the t-th token.</li> <li> <strong>Value Update</strong>: The update to the query token using the attention mechanism is: \(e_{T+1,\text{new}} = \sum_{t=1}^{T} \alpha_t v_t\) Where \(v_t\) is the value for the t-th token.</li> </ol> <p>Given the third condition, the value for each token is: \(v_t = P W_v e_t\) Substituting this in, we get: \(e_{T+1,\text{new}} = \sum_{t=1}^{T} \alpha_t P W_v e_t\)</p> <p><strong>Matching with Gradient Descent</strong>: Given the attention weights \((\alpha_t)\) and the gradient descent update rule, the y-component of the updated query token is: \(e_{T+1,\text{new,y}} = \sum_{t=1}^{T} \alpha_t (-\eta y_{\tau,i} + \eta W_0 x_{\tau,i}) \\ e_{T+1,\text{new,y}} = \sum_{t=1}^{T} \alpha_t \eta (W_0 x_{\tau,i} - y_{\tau,i})\)</p> <p>Now, comparing this with the gradient descent update rule, we can see that the term inside the summation \((W_0x_œÑ,_iy_œÑ,_i)\) is essentially the error in prediction for the training data.</p> <p>Given that the attention weights <em>\(Œ±_t\)</em> effectively compute a weighted version of this error, the entire equation can be seen as a weighted sum of errors, which is analogous to the gradient in gradient descent.</p> <h3 id="approximation">Approximation:</h3> <hr> <p>Now, for the test input \(x_{\tau,\text{test}}\), the prediction using the initial weights \(W_0\) is \(W_0 x_{\tau,\text{test}}\). After one step of gradient descent, the prediction becomes: \((W_0 - \Delta W_0) x_{\tau,\text{test}}\)</p> <p>Given that the self-attention mechanism‚Äôs update to the query token is designed to mimic one step of gradient descent, we can approximate: \(e_{T+1,\text{new,y}} \approx (W_0 - \Delta W_0) x_{\tau,\text{test}}\)</p> <p>This approximation captures the essence of the theoretical construction: the self-attention mechanism updates the query token in a way that mimics the behavior of gradient descent</p> <p>Also the rearrangement and approximation are based on the insight that the self-attention mechanism‚Äôs update to the query token aligns with the gradient descent update rule for linear regression, allowing the mechanism to make predictions consistent with gradient descent optimization.</p> </div> </article> </div> </div> <div id="contactBtn" onclick="toggleForm()"> Let's Chat üí¨ </div> <div id="contactForm"> <div class="formHeader"> <h2>Have a thought? Let's chat!</h2> <button onclick="toggleForm()" class="closeBtn">x</button> </div> <form action="https://formspree.io/f/mdorylbw" method="post"> <label for="email">Email</label> <input type="email" id="email" name="email" placeholder="Enter your email"> <label for="name">Name</label> <input type="text" id="name" name="name" placeholder="Enter your name"> <label for="message">Message</label> <textarea id="message" name="message" placeholder="Enter your message"></textarea> <input type="submit" value="SEND"> </form> </div> <script>function toggleForm(){const t=document.getElementById("contactForm");"translateY(0%)"===t.style.transform?t.style.transform="translateY(100%)":t.style.transform="translateY(0%)"}</script> <footer class="fixed-bottom"> <div class="container mt-0 text-center"> ¬© Copyright 2023 Swayam Singh. Last updated: October 24, 2023. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-HJEMV6MXV1"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-HJEMV6MXV1");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>